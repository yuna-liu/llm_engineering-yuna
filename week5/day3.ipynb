{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe37963-1af6-44fc-a841-8e462443f5e6",
   "metadata": {},
   "source": [
    "## Expert Knowledge Worker\n",
    "\n",
    "### A question answering agent that is an expert knowledge worker\n",
    "### To be used by employees of Insurellm, an Insurance Tech company\n",
    "### The agent needs to be accurate and the solution should be low cost.\n",
    "\n",
    "This project will use RAG (Retrieval Augmented Generation) to ensure our question/answering assistant has high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba2779af-84ef-4227-9e9e-6eaf0df87e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YLIU378\\Programs\\Python\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06f0cb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (0.3.28)\n",
      "Requirement already satisfied: langchain-chroma in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (0.2.4)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.68 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from langchain-openai) (0.3.68)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.86.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from langchain-openai) (1.95.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from langchain-chroma) (2.3.1)\n",
      "Requirement already satisfied: chromadb>=1.0.9 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from langchain-chroma) (1.0.15)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.0-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/60.8 kB ? eta -:--:--\n",
      "     -------------------------------- ----- 51.2/60.8 kB 890.4 kB/s eta 0:00:01\n",
      "     -------------------------------- ----- 51.2/60.8 kB 890.4 kB/s eta 0:00:01\n",
      "     -------------------------------- ----- 51.2/60.8 kB 890.4 kB/s eta 0:00:01\n",
      "     -------------------------------- ----- 51.2/60.8 kB 890.4 kB/s eta 0:00:01\n",
      "     -------------------------------------- 60.8/60.8 kB 231.7 kB/s eta 0:00:00\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from chromadb>=1.0.9->langchain-chroma) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from chromadb>=1.0.9->langchain-chroma) (2.10.6)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from chromadb>=1.0.9->langchain-chroma) (1.4.1)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (0.35.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from chromadb>=1.0.9->langchain-chroma) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from chromadb>=1.0.9->langchain-chroma) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from chromadb>=1.0.9->langchain-chroma) (1.22.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from chromadb>=1.0.9->langchain-chroma) (1.35.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from chromadb>=1.0.9->langchain-chroma) (1.35.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from chromadb>=1.0.9->langchain-chroma) (1.35.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from chromadb>=1.0.9->langchain-chroma) (0.21.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from chromadb>=1.0.9->langchain-chroma) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from chromadb>=1.0.9->langchain-chroma) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from chromadb>=1.0.9->langchain-chroma) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from chromadb>=1.0.9->langchain-chroma) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from chromadb>=1.0.9->langchain-chroma) (1.73.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from chromadb>=1.0.9->langchain-chroma) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from chromadb>=1.0.9->langchain-chroma) (0.16.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from chromadb>=1.0.9->langchain-chroma) (33.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from chromadb>=1.0.9->langchain-chroma) (9.1.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from chromadb>=1.0.9->langchain-chroma) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from chromadb>=1.0.9->langchain-chroma) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from chromadb>=1.0.9->langchain-chroma) (3.10.18)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from chromadb>=1.0.9->langchain-chroma) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from chromadb>=1.0.9->langchain-chroma) (14.0.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from chromadb>=1.0.9->langchain-chroma) (4.24.0)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (0.4.5)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (23.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.31.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.86.0->langchain-openai) (2.10)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from build>=1.0.3->chromadb>=1.0.9->langchain-chroma) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from build>=1.0.3->chromadb>=1.0.9->langchain-chroma) (0.4.6)\n",
      "Requirement already satisfied: certifi in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.68->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (0.25.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (3.3.1)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.2.1)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (0.10)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-openai) (0.23.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (25.2.10)\n",
      "Requirement already satisfied: protobuf in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (6.31.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain-chroma) (8.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.35.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma) (1.35.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.35.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma) (1.35.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.56b0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb>=1.0.9->langchain-chroma) (0.56b0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.9->langchain-chroma) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain-chroma) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain-chroma) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.3.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain-chroma) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain-chroma) (2.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (0.33.4)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from typer>=0.9.0->chromadb>=1.0.9->langchain-chroma) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from typer>=0.9.0->chromadb>=1.0.9->langchain-chroma) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (1.1.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (15.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (4.9.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (3.12.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (2025.5.1)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain-chroma) (3.23.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=1.0.9->langchain-chroma) (0.1.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (0.6.1)\n",
      "Downloading scikit_learn-1.7.0-cp312-cp312-win_amd64.whl (10.7 MB)\n",
      "   ---------------------------------------- 0.0/10.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/10.7 MB 5.2 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.8/10.7 MB 8.7 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.7/10.7 MB 12.0 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.5/10.7 MB 13.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.6/10.7 MB 15.2 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.5/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.2/10.7 MB 15.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.0/10.7 MB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.9/10.7 MB 16.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.7/10.7 MB 16.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.5/10.7 MB 17.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.2/10.7 MB 16.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.1/10.7 MB 17.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.7/10.7 MB 18.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.7/10.7 MB 17.2 MB/s eta 0:00:00\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "   ---------------------------------------- 0.0/307.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 307.7/307.7 kB 18.6 MB/s eta 0:00:00\n",
      "Downloading scipy-1.16.0-cp312-cp312-win_amd64.whl (38.4 MB)\n",
      "   ---------------------------------------- 0.0/38.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/38.4 MB 18.6 MB/s eta 0:00:03\n",
      "    --------------------------------------- 0.9/38.4 MB 10.9 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 2.1/38.4 MB 17.0 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 3.0/38.4 MB 17.3 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 3.7/38.4 MB 17.0 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 4.5/38.4 MB 16.9 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 5.3/38.4 MB 16.9 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 6.2/38.4 MB 17.1 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 7.0/38.4 MB 17.9 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 7.9/38.4 MB 18.0 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 8.8/38.4 MB 18.1 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 9.6/38.4 MB 18.1 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 10.4/38.4 MB 17.7 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 11.4/38.4 MB 19.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 12.3/38.4 MB 18.7 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 13.1/38.4 MB 18.2 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 13.2/38.4 MB 17.2 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 14.9/38.4 MB 18.7 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 15.8/38.4 MB 18.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 16.7/38.4 MB 19.3 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 17.5/38.4 MB 19.3 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 18.2/38.4 MB 18.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 19.0/38.4 MB 18.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 20.0/38.4 MB 18.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 20.7/38.4 MB 19.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 21.6/38.4 MB 18.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 22.4/38.4 MB 18.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 23.4/38.4 MB 20.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 24.2/38.4 MB 19.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 25.1/38.4 MB 18.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 25.9/38.4 MB 18.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 26.9/38.4 MB 18.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 27.6/38.4 MB 18.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 28.6/38.4 MB 18.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 29.3/38.4 MB 18.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 30.2/38.4 MB 18.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 31.1/38.4 MB 18.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 32.0/38.4 MB 18.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 32.9/38.4 MB 18.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 33.7/38.4 MB 19.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 34.7/38.4 MB 18.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 35.5/38.4 MB 18.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 36.4/38.4 MB 18.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.4/38.4 MB 19.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.4/38.4 MB 19.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.4/38.4 MB 19.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.4/38.4 MB 19.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.4/38.4 MB 16.4 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.1 scikit-learn-1.7.0 scipy-1.16.0 threadpoolctl-3.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-openai langchain-chroma scikit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13a1a608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Found existing installation: numpy 2.3.1\n",
      "Uninstalling numpy-2.3.1:\n",
      "  Successfully uninstalled numpy-2.3.1\n",
      "Found existing installation: pandas 2.3.1\n",
      "Uninstalling pandas-2.3.1:\n",
      "  Successfully uninstalled pandas-2.3.1\n",
      "Found existing installation: scikit-learn 1.7.0\n",
      "Uninstalling scikit-learn-1.7.0:\n",
      "  Successfully uninstalled scikit-learn-1.7.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\YLIU378\\Programs\\Python\\Lib\\site-packages\\~klearn'.\n",
      "You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall numpy pandas scikit-learn -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e1616f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.3.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.9 kB ? eta -:--:--\n",
      "     ------------ ------------------------- 20.5/60.9 kB 320.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 60.9/60.9 kB 641.1 kB/s eta 0:00:00\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.1-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yliu378\\programs\\python\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading numpy-2.3.1-cp312-cp312-win_amd64.whl (12.7 MB)\n",
      "   ---------------------------------------- 0.0/12.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/12.7 MB 6.3 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.7/12.7 MB 7.3 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 1.2/12.7 MB 9.7 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 2.1/12.7 MB 12.5 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 2.9/12.7 MB 13.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.6/12.7 MB 13.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 4.2/12.7 MB 13.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 4.8/12.7 MB 13.4 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 5.6/12.7 MB 13.8 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 6.3/12.7 MB 13.9 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.8/12.7 MB 14.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 8.0/12.7 MB 14.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.7/12.7 MB 14.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.4/12.7 MB 14.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.3/12.7 MB 15.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.0/12.7 MB 16.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.8/12.7 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.7 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.7/12.7 MB 16.4 MB/s eta 0:00:00\n",
      "Downloading pandas-2.3.1-cp312-cp312-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/11.0 MB 25.1 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 1.4/11.0 MB 17.5 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.9/11.0 MB 15.5 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 2.0/11.0 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.6/11.0 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.4/11.0 MB 16.6 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.3/11.0 MB 17.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.5/11.0 MB 16.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.7/11.0 MB 16.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.3/11.0 MB 16.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.2/11.0 MB 16.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.0/11.0 MB 16.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.6/11.0 MB 16.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.5/11.0 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 15.9 MB/s eta 0:00:00\n",
      "Downloading scikit_learn-1.7.0-cp312-cp312-win_amd64.whl (10.7 MB)\n",
      "   ---------------------------------------- 0.0/10.7 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.7/10.7 MB 23.1 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 1.1/10.7 MB 16.9 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.4/10.7 MB 22.4 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 3.3/10.7 MB 19.2 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 4.1/10.7 MB 18.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.9/10.7 MB 18.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.7/10.7 MB 18.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.5/10.7 MB 18.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.0/10.7 MB 17.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.7/10.7 MB 17.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.5/10.7 MB 17.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.5/10.7 MB 17.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.1/10.7 MB 17.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.7/10.7 MB 17.2 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy, pandas, scikit-learn\n",
      "Successfully installed numpy-2.3.1 pandas-2.3.1 scikit-learn-1.7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.1.1 requires numpy<2.0,>=1.26.0rc1; python_version >= \"3.12\", but you have numpy 2.3.1 which is incompatible.\n",
      "matplotlib 3.8.0 requires numpy<2,>=1.21, but you have numpy 2.3.1 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --no-cache-dir numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "802137aa-8a74-45e0-a487-d1974927d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain and Chroma and plotly\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58c85082-e417-4708-9efe-81a5d55d1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price is a factor for our company, so we're going to use a low cost model\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee78efcb-60fe-449e-a944-40bab26261af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "730711a9-6ffe-4eee-8f48-d6cfb7314905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in documents using LangChain's loaders\n",
    "# Take everything in all the sub-folders of our knowledgebase\n",
    "\n",
    "folders = glob.glob(\"knowledge-base/*\")\n",
    "\n",
    "# With thanks to CG and Jon R, students on the course, for this fix needed for some users \n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "# If that doesn't work, some Windows users might need to uncomment the next line instead\n",
    "# text_loader_kwargs={'autodetect_encoding': True}\n",
    "\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "    folder_docs = loader.load()\n",
    "    for doc in folder_docs:\n",
    "        doc.metadata[\"doc_type\"] = doc_type\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f065d4b1-80b7-4e15-abd4-60a83e752ea8",
   "metadata": {},
   "source": [
    "# Please note:\n",
    "\n",
    "In the next cell, we split the text into chunks.\n",
    "\n",
    "2 students let me know that the next cell crashed their computer.  \n",
    "They were able to fix it by changing the chunk_size from 1,000 to 2,000 and the chunk_overlap from 200 to 400.  \n",
    "This shouldn't be required; but if it happens to you, please make that change!  \n",
    "(Note that LangChain may give a warning about a chunk being larger than 1,000 - this can be safely ignored).\n",
    "\n",
    "_With much thanks to Steven W and Nir P for this valuable contribution._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7310c9c8-03c1-4efc-a104-5e89aec6db1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1088, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd06e02f-6d9b-44cc-a43d-e1faa8acc7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c54b4b6-06da-463d-bee7-4dd456c2b887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document types found: products, employees, contracts, company\n"
     ]
    }
   ],
   "source": [
    "doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)\n",
    "print(f\"Document types found: {', '.join(doc_types)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f7d2a6-ccfa-425b-a1c3-5e55b23bd013",
   "metadata": {},
   "source": [
    "## A sidenote on Embeddings, and \"Auto-Encoding LLMs\"\n",
    "\n",
    "We will be mapping each chunk of text into a Vector that represents the meaning of the text, known as an embedding.\n",
    "\n",
    "OpenAI offers a model to do this, which we will use by calling their API with some LangChain code.\n",
    "\n",
    "This model is an example of an \"Auto-Encoding LLM\" which generates an output given a complete input.\n",
    "It's different to all the other LLMs we've discussed today, which are known as \"Auto-Regressive LLMs\", and generate future tokens based only on past context.\n",
    "\n",
    "Another example of an Auto-Encoding LLMs is BERT from Google. In addition to embedding, Auto-encoding LLMs are often used for classification.\n",
    "\n",
    "### Sidenote\n",
    "\n",
    "In week 8 we will return to RAG and vector embeddings, and we will use an open-source vector encoder so that the data never leaves our computer - that's an important consideration when building enterprise systems and the data needs to remain internal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78998399-ac17-4e28-b15f-0b5f51e6ee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# If you would rather use the free Vector Embeddings from HuggingFace sentence-transformers\n",
    "# Then replace embeddings = OpenAIEmbeddings()\n",
    "# with:\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "763e51ff-5787-4a56-8176-36b7c5796fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a Chroma Datastore already exists - if so, delete the collection to start from scratch\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99fe3a37-480f-4d55-be48-120588d5846b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: your_ope*******_key. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create our Chroma vectorstore!\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdb_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVectorstore created with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvectorstore\u001b[38;5;241m.\u001b[39m_collection\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m documents\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\YLIU378\\Programs\\Python\\Lib\\site-packages\\langchain_chroma\\vectorstores.py:1234\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[1;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1233\u001b[0m     ids \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mid \u001b[38;5;28;01mif\u001b[39;00m doc\u001b[38;5;241m.\u001b[39mid \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(uuid\u001b[38;5;241m.\u001b[39muuid4()) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m-> 1234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1236\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1244\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1245\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\YLIU378\\Programs\\Python\\Lib\\site-packages\\langchain_chroma\\vectorstores.py:1187\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m   1179\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[0;32m   1182\u001b[0m         api\u001b[38;5;241m=\u001b[39mchroma_collection\u001b[38;5;241m.\u001b[39m_client,\n\u001b[0;32m   1183\u001b[0m         ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[0;32m   1184\u001b[0m         metadatas\u001b[38;5;241m=\u001b[39mmetadatas,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1185\u001b[0m         documents\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[0;32m   1186\u001b[0m     ):\n\u001b[1;32m-> 1187\u001b[0m         \u001b[43mchroma_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1188\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1189\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m   1190\u001b[0m \u001b[43m            \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1193\u001b[0m     chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(texts\u001b[38;5;241m=\u001b[39mtexts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mids)\n",
      "File \u001b[1;32mc:\\Users\\YLIU378\\Programs\\Python\\Lib\\site-packages\\langchain_chroma\\vectorstores.py:527\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    525\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[0;32m    526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 527\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[0;32m    531\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "File \u001b[1;32mc:\\Users\\YLIU378\\Programs\\Python\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:590\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[1;34m(self, texts, chunk_size, **kwargs)\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[0;32m    589\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[1;32m--> 590\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\YLIU378\\Programs\\Python\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:478\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size, **kwargs)\u001b[0m\n\u001b[0;32m    476\u001b[0m batched_embeddings: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[1;32m--> 478\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mclient_kwargs\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    482\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[1;32mc:\\Users\\YLIU378\\Programs\\Python\\Lib\\site-packages\\openai\\resources\\embeddings.py:132\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[1;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    126\u001b[0m             embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[0;32m    127\u001b[0m                 base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    128\u001b[0m             )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\YLIU378\\Programs\\Python\\Lib\\site-packages\\openai\\_base_client.py:1256\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1244\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1251\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1252\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1253\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1254\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1255\u001b[0m     )\n\u001b[1;32m-> 1256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\YLIU378\\Programs\\Python\\Lib\\site-packages\\openai\\_base_client.py:1044\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1043\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1044\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: your_ope*******_key. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "# Create our Chroma vectorstore!\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "057868f6-51a6-4087-94d1-380145821550",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorstore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Get one vector and find how many dimensions it has\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m collection \u001b[38;5;241m=\u001b[39m \u001b[43mvectorstore\u001b[49m\u001b[38;5;241m.\u001b[39m_collection\n\u001b[0;32m      4\u001b[0m sample_embedding \u001b[38;5;241m=\u001b[39m collection\u001b[38;5;241m.\u001b[39mget(limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m])[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      5\u001b[0m dimensions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(sample_embedding)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vectorstore' is not defined"
     ]
    }
   ],
   "source": [
    "# Get one vector and find how many dimensions it has\n",
    "\n",
    "collection = vectorstore._collection\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"The vectors have {dimensions:,} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e393a0-dd4c-419f-842f-60c1cb3b716b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0d45462-a818-441c-b010-b85b32bcf618",
   "metadata": {},
   "source": [
    "## Visualizing the Vector Store\n",
    "\n",
    "Let's take a minute to look at the documents and their embedding vectors to see what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98adf5e-d464-4bd2-9bdf-bc5b6770263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prework\n",
    "\n",
    "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "vectors = np.array(result['embeddings'])\n",
    "documents = result['documents']\n",
    "doc_types = [metadata['doc_type'] for metadata in result['metadatas']]\n",
    "colors = [['blue', 'green', 'red', 'orange'][['products', 'employees', 'contracts', 'company'].index(t)] for t in doc_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427149d5-e5d8-4abd-bb6f-7ef0333cca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We humans find it easier to visalize things in 2D!\n",
    "# Reduce the dimensionality of the vectors to 2D using t-SNE\n",
    "# (t-distributed stochastic neighbor embedding)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 2D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='2D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x',yaxis_title='y'),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1418e88-acd5-460a-bf2b-4e6efc88e3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try 3D!\n",
    "\n",
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 3D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    z=reduced_vectors[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='3D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),\n",
    "    width=900,\n",
    "    height=700,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3ada26-b4b7-42fc-b943-933c14adf89b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
